# -*- coding: utf-8 -*-
"""EyeRecycleRNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15k1RPTeUkLJg0cekAF65dwlcki_MQ4uQ
"""

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext autoreload
# %autoreload 2
# %matplotlib inline

# %config InlineBackend.figure_format = 'retina'

from fastai.vision import *
from fastai.metrics import error_rate
from pathlib import Path
from glob2 import glob
from sklearn.metrics import confusion_matrix

import torch
import pandas as pd
import numpy as np
import os
import zipfile as zf
import shutil
import re
import seaborn as sns

"""Upload dataset here"""

# Commented out IPython magic to ensure Python compatibility.
# %rm -rf Waste-images.zip
# %rm -rf Waste-images/
# %rm -rf data/

from google.colab import files
uploaded = files.upload()

files = zf.ZipFile("Waste-images.zip",'r')
files.extractall()
files.close()

os.listdir(os.path.join(os.getcwd(),"Waste Images"))

"""The following will split the data into train, test anbd validation folders respectiveley at random"""

def split_indices(folder,seed1,seed2):    
    n = len(os.listdir(folder))
    full_set = list(range(1,n+1))
    
    #Train Function
    random.seed(seed1)
    train = random.sample(list(range(1,n+1)),int(.5*n))

    remain = list(set(full_set)-set(train))

    #Seperate leftover data into validation and test folders
    random.seed(seed2)
    valid = random.sample(remain,int(.5*len(remain)))
    test = list(set(remain)-set(valid))
    
    return(train,valid,test)

"""Seperate leftover data into validation and test folders

Finds the file name for specific images.
"""

def get_names(waste_type,indices):
    file_names = [waste_type+str(i)+".jpg" for i in indices]
    return(file_names)

"""Moves images from original folders into the newlyu created destination folder"""

def move_files(source_files,destination_folder):
    for file in source_files:
        shutil.move(file,destination_folder)

"""Specifies path name (e.g. Train/Paper, Validate/glass etc."""

subsets = ['train','valid']
waste_types = ['cardboard','glass','metal','paper','plastic','trash']

"""Creates a new folder for each data subset"""

for subset in subsets:
    for waste_type in waste_types:
        folder = os.path.join('data',subset,waste_type)
        if not os.path.exists(folder):
            os.makedirs(folder)

if not os.path.exists(os.path.join('data','test')):
    os.makedirs(os.path.join('data','test'))

"""Moves images to data subset folders."""

for waste_type in waste_types:
    source_folder = os.path.join('dataset-resized',waste_type)
    train_ind, valid_ind, test_ind = split_indices(source_folder,1,1)

    #Move original files to train 
    train_names = get_names(waste_type,train_ind)
    train_source_files = [os.path.join(source_folder,name) for name in train_names]
    train_dest = "data/train/"+waste_type
    move_files(train_source_files,train_dest) 

    #Move original files to validate
    valid_names = get_names(waste_type,valid_ind)
    valid_source_files = [os.path.join(source_folder,name) for name in valid_names]
    valid_dest = "data/valid/"+waste_type
    move_files(valid_source_files,valid_dest)

    #Move original files to test
    test_names = get_names(waste_type,test_ind)
    test_source_files = [os.path.join(source_folder,name) for name in test_names]
    move_files(test_source_files,"data/test")

"""Defines path to get the image folder"""

path = Path(os.getcwd())/"data"
path

"""Flips the image vertically & horizontally, sets batch size """

tfms = get_transforms(do_flip=True,flip_vert=True)
data = ImageDataBunch.from_folder(path,test="test",ds_tfms=tfms,bs=16)

"""Show overview of data using ImageDataBunch;"""

data

"""Displays image labels to check it has been done correctly"""

print(data.classes)

"""Shows one batch of data, according to batch size"""

data.show_batch(rows=4,figsize=(10,8))

"""Defines neural network parameters, and measurable metric"""

learn = create_cnn(data,models.resnet34,metrics=accuracy)

"""Learn model structure, prepare batches. """

learn.model

"""Used to find the appropriate learnign rate, assess gradient and find optimal set of weights."""

learn.lr_find(start_lr=1e-6,end_lr=1e1)
learn.recorder.plot()

learn.recorder.plot()

"""Begin the learnign cycle, the number in () is the number of epochs the model will run through  """

learn.fit_one_cycle(30)

interp = ClassificationInterpretation.from_learner(learn)
losses,idxs = interp.top_losses()

interp.plot_top_losses(9, figsize=(15,11))

doc(interp.plot_top_losses)
interp.plot_confusion_matrix(figsize=(12,12), dpi=60)

interp.most_confused(min_val=2)

preds = learn.get_preds(ds_type=DatasetType.Test)

print(preds[0].shape)
preds[0]

data.classes

max_idxs = np.asarray(np.argmax(preds[0],axis=1))

yhat = []
for max_idx in max_idxs:
  yhat.append(data.classes[max_idx])

yhat

learn.data.test_ds[0][0]

y = []

## convert POSIX paths to string first
for label_path in data.test_ds.items:
    y.append(str(label_path))
    
## then extract waste type from file path
pattern = re.compile("([a-z]+)[0-9]+")
for i in range(len(y)):
    y[i] = pattern.search(y[i]).group(1)

## predicted values
print(yhat[0:5])
## actual values
print(y[0:5])

learn.data.test_ds[0][0]

"""# New section"""

cm = confusion_matrix(y,yhat)
print(cm)

df_cm = pd.DataFrame(cm,waste_types,waste_types)

plt.figure(figsize=(10,8))
sns.heatmap(df_cm,annot=True,fmt="d",cmap="YlGnBu")

correct = 0

for r in range(len(cm)):
    for c in range(len(cm)):
        if (r==c):
            correct += cm[r,c]

accuracy = correct/sum(sum(cm))
accuracy

"""91% accuracy achieved on this dataset, this is an extremely good score, most models of this nature may achieve on average 65% accuracy, further improvement may erradicate future errors. """